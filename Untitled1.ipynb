{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(r'C:\\Users\\mahendra\\Desktop\\analytics\\projects\\Janatahack Independence Day 2020 ML Hackathon\\train.csv')\n",
    "test_data=pd.read_csv(r'C:\\Users\\mahendra\\Desktop\\analytics\\projects\\Janatahack Independence Day 2020 ML Hackathon\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_col'] = train_data['TITLE'].str.cat(train_data['ABSTRACT'],sep=\" \")\n",
    "test_data['text_col'] = test_data['TITLE'].str.cat(test_data['ABSTRACT'],sep=\" \")\n",
    "\n",
    "train_data1=train_data.drop(columns=['TITLE','ABSTRACT'])\n",
    "test_data1=test_data.drop(columns=['TITLE','ABSTRACT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_data1['text_col']\n",
    "y_train=train_data1.drop(columns=['text_col','ID'])\n",
    "\n",
    "X_test=test_data1['text_col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess fuction\n",
    "\n",
    "#Text Lowercase:\n",
    "def text_lowercase(text): \n",
    "    return text.lower()\n",
    "\n",
    "# Remove numbers \n",
    "def remove_numbers(text): \n",
    "    result = re.sub(r'\\d+', '', text) \n",
    "    return result\n",
    "#Remove punctuation\n",
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator)\n",
    "# remove whitespace from text \n",
    "def remove_whitespace(text): \n",
    "    return  \" \".join(text.split())\n",
    "# Remove default stopwords,Stemming,Lemmatization\n",
    "def stop_words(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    stop_words_remove = [word for word in word_tokens if word not in stop_words] #Remove default stopwords\n",
    "    filtered_word= [WordNetLemmatizer().lemmatize(word, pos ='v') for word in stop_words_remove]\n",
    "    return filtered_word\n",
    "def remove_duplicate(list_word):\n",
    "    return (list(set(list_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(text):\n",
    "    text_lower=text_lowercase(text)\n",
    "    text_num=remove_numbers(text_lower)\n",
    "    text_puc=remove_punctuation(text_num)\n",
    "    text_wht=remove_whitespace(text_puc)\n",
    "    text_st=stop_words(text_wht)\n",
    "    text_rm=remove_duplicate(text_st)\n",
    "    return(text_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]\n",
    "    X_indices =np.zeros((m,max_len))\n",
    "    \n",
    "    for i in range(m):                             # loop over training examples\n",
    "        sentence_words =pre_processing(X[i])\n",
    "        j = 0  # Initialize j to 0\n",
    "        for w in sentence_words:\n",
    "            if w not in list(word_to_index.keys()):\n",
    "                w= 'unkown_word'\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j = j+1\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open('glove.6B.300d.txt',encoding=\"utf8\") as f: \n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "word_to_vec_map=pd.DataFrame(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_map['unkown_word']=word_to_vec_map.apply(lambda x: 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vocab of word\n",
    "vocab=list(word_to_vec_map.columns.tolist())\n",
    "word_to_index={k: v for v, k in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    emb_matrix =np.zeros((vocab_len,emb_dim))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_len,emb_dim,trainable=True)\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings =embedding_layer(sentence_indices)\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # The returned output should be a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(.5)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(.5)(X)\n",
    "    # Propagate X through a Dense layer with 5 units\n",
    "    X =Dense(6)(X)\n",
    "    # Add a softmax activation\n",
    "    X =Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model =Model(sentence_indices, X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 462)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 462, 300)          120000600 \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 462, 128)          219648    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 462, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 120,352,606\n",
      "Trainable params: 120,352,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = classifier_model((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_oh=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
